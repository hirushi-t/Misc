{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## **Generative Adverserial Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "A GAN is a machine learning framework where two neural networks compete against each other in a zero sum game (i.e one player's win is another player's loss). These 2 components are the:\n",
    "1) **Generator:** create new data that is like the training data\n",
    "2) **Discriminator:** a binary classifier that detects if the generated data is real or fake.\n",
    "\n",
    "The 2 components are in competition: the generator strives to produce data that increasingly resembles real data, making it harder for the discriminator to distinguish between real and fake. Meanwhile, the discriminator continously improves its ability to detect fakes more accurately.\n",
    "\n",
    "Originally, this was proposed as a form of generative model for unsupervised learning. However, GANs have proved useful for semi-supervised learning, fully supervised learning and reinforcement learning.\n",
    "\n",
    "We train the model 'indirectly' through the discriminator. So, the generator is not trained to minimise the distance to a specific image, rather to fool the discriminator. This enables the model to learn in an unsupervise way.\n",
    "\n",
    "GANs do not explicitly model the likelihood function nor provides a means for finding the latent variable corresponding to a given sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../images/GAN_illustration.png\" alt=\"Description\" width=\"200\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Learning Objectives**\n",
    "The generative network's training objective is to increase the error rate of the discriminative network. The discriminative network's training objective is to do the opposite. Because the discriminator is a binary classifier, we can use the binary cross entropy (BCE)\n",
    "\n",
    "#### **In terms of distributions...**\n",
    "Typically, the input to the generator will be a random point in latent space. For example, a sample from a multivariate normal distribution. So, the generator will learn the mapping from this latent space to a data distribution of interest. This means that GANs are able to learn the underlying distribution implicitly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here is a graph breaking down different type of generative models.*\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../images/taxonomy_of_gen_models.png\" alt=\"Description\" width=\"700\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Common Issues:**\n",
    "- Non-convergence: the loss oscillates and never converges\n",
    "    - Discriminator loss goes to 0 so generator gradient vanishes and learns nothing. \n",
    "    - Generator loss stays high\n",
    "    - Cost functions may not converge in a minimax game.\n",
    "- Vanishing gradients\n",
    "- Saturation - the dscriminator and generator loss stops decrease and model stops learning.\n",
    "- Modal collapse. For example, the MNIST dataset has 10 major modes (digits 0 to 0). This problem occurs when only a few modes of data are generated. \n",
    "- Very sensitive to hyperparmaeter selections\n",
    "- Slow training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Possible solutions:**\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONTINUE WITH PROBLEMS AND POSSIBLE SOLUTIONS\n",
    "\n",
    "https://medium.com/towards-data-science/gan-ways-to-improve-gan-performance-acf37f9f59b\n",
    "\n",
    "https://jonathan-hui.medium.com/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
