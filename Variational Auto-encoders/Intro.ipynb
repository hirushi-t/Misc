{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Variational Auto-encoders (VAE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional autoencoders, given an input, generates a fixed point in latent space. This lower-dimensional representation  captures the key features of the input data, enabling reconstruction back to the original input in the decoding component of your auto-encoder.\n",
    "\n",
    "**A variational auto-encoder is a probabilistic approach to auto-encoders.**\n",
    "\n",
    "\n",
    "#### **Why do we need VAEs?**\n",
    "While traditional autoencodoers are great for tasks like dimensionality reduction and feature learning, they have significant limitations when used as generative models. Here's why:\n",
    "- **Latent space irregularity:** the encoder maps input data to a lower dimensional latent space. However, this mapping is not structured or regularised. Therfore, the latent space is disorganised, discontinous and can be sparse. For a generative model, we would sample a point from the latent space and pass it through the decoder. Due to the disorganised nature of the latent space, there is no guarantee that the output will be meaningul and resemble the true data. This is because the encoder only maps input data to specific regions of the latent space during training.\n",
    "- **No probabilistic framework:** standard encoders do not model the underlying distribution of the data. They simply learn a deterministic mapping from input to latent space and back. Without a probabilistic framework, it can be dificult to generate new data points that are diverse and fully representative of the training data. \n",
    "\n",
    "*Note, a **structured** latent space means that similar inputs should have similar latent representations. In a standard auto-encoder, this is not necessarily the case - points in the latent space may be scattered, and there is no guarantee that similar datapoints will be close to each other.*\n",
    "\n",
    "*Note, **regularisation** involves adding contraints and penalties to encourage desirable properties in the latent space, such as continouity or smoothness.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Basic Idea**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let:\n",
    "- $p(x)$ be the data distribution\n",
    "- $p(z)$ be the latent distribution\n",
    "\n",
    "The posterior distribution is therefore given by: \n",
    "$$p(z|x)$$\n",
    "\n",
    "The likelihood is given by:\n",
    "$$p(x|z)$$\n",
    "\n",
    "If we can sample from the posterior distribution, we will obtain points in the latent space that is likely to have been generated from our original data distribution.\n",
    "\n",
    "**Problem:** we don't know the exact form of the latent distributions $p(z)$, which make the computations intractable\n",
    "\n",
    "**Solution:** we assume the latent distribution $p(z)$ is a normal distribution. \n",
    "\n",
    "Now for the 'variational' part. Additionally, since we don't know the exact form of the posterior distribution, we also estimate that to be Gaussian:\n",
    "$$\n",
    "p(z|x) \\approx q(z|x) = \\mathcal{N} (\\mu, \\sigma)\n",
    "$$\n",
    "We will train a deep encoder to learn the parameters $\\mu$ and $\\sigma$.\n",
    "\n",
    "Then, we will use a decoder to contruct images/data from samples of the approximate posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to train the autoencoder?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a variational inference problem, we use the **ELBO function:**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x) = \\mathbb{E}_{q(z|x)} \\left[ \\log p(x|z) \\right] - \\text{KL}(q(z|x) \\, \\| \\, p(z))\n",
    "$$\n",
    "\n",
    "\n",
    "CONTINUE FROM HERE....\n",
    "\n",
    "The first term is a data-fit term and the second is a regularisation term. Conveniently, due to the choice of Gaussian prior and Gaussian approximate posterior, maximising the data-fit term correspond to minimising the MSE. \n",
    "\n",
    "https://www.youtube.com/watch?v=qJeaCHQ1k2w\n",
    "https://www.youtube.com/watch?v=7Pcvdo4EJeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
