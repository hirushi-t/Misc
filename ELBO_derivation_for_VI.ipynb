{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## **Evidence Lower Bound Objective function (ELBO)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **ELBO** is the key objective function in **variational inference** (VI).\n",
    "- Variational inference is a machine learning method that approximates complex probability distributions, particularly posterior distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Derivation of ELBO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the marginal likelihood $p(x)$, also known as evidence. The marginal likelihood is the likelihood function that has been integrated over the latent space. It is the distribution of the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Marginalise the log density of the data distribution:\n",
    "$$\n",
    "log(p(x)) = log \\int p(x, z) \\, dz\n",
    "$$\n",
    "\n",
    "2) Introduce the approximate posterior using a simple trick.\n",
    "\\begin{align*}\n",
    "\\log p(x) &= \\log \\int \\frac{q(z|x)}{q(z|x)} p(x, z) \\, dz \\\\\n",
    "&= \\log \\, \\mathbb{E}_{q(z|x)} \\left [\\frac{p(x,z)}{q(z|x)} \\right ]\n",
    "\\end{align*}\n",
    "\n",
    "3) The log function is concave therefore we can apply **Jensen's inequality**:\n",
    "$$\n",
    "\\mathbb{E}[f(x)] \\geq f(\\mathbb{E}[x])\n",
    "$$\n",
    "\n",
    "4) Applying Jensen's inequality gives:\n",
    "\n",
    "\\begin{align*}\n",
    "\\log p(x) \\geq  \\mathbb{E}[x]_{q(z|x)} \\left [\\log \\frac{p(x,z)}{q(z|x)} \\right ] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The right hand side is known as the **evidence lower bound**. The ELBO is a lower bound on the log marginal likelihood of the data. By **maximising ELBO**, we are indirectly maximising the log marginal likelihood, which is the ultimate goal of generative modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further decomposition:\n",
    "\n",
    "\\begin{align*}\n",
    "\\log p(x) &\\geq \\mathbb{E}_{q(z|x)} \\left[ \\log \\frac{p(x, z)}{q(z|x)} \\right] \\\\\n",
    "          &= \\mathbb{E}_{q(z|x)} \\left[ \\log p(x, z) \\right] - \\mathbb{E}_{q(z|x)} \\left[ \\log q(z|x) \\right] \\\\\n",
    "          &= \\mathbb{E}_{q(z|x)} \\left[ \\log p(x|z) \\right] + \\mathbb{E}_{q(z|x)} \\left[ \\log p(z) \\right] - \\mathbb{E}_{q(z|x)} \\left[ \\log q(z|x) \\right] \\\\\n",
    "          &= \\mathbb{E}_{q(z|x)} \\left[ \\log p(x|z) \\right] - \\text{KL}(q(z|x) \\, \\| \\, p(z)),\n",
    "\\end{align*}\n",
    "\n",
    "This further decomposion of ELBO shows that maximising the ELBO involves minimising the KL divergence between the approximate posterior and the prior (the second term), whilst maximising the reconstruction accuracy (the first term)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
