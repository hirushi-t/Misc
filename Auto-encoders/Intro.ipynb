{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Auto-encoders**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An auto-encoder is used to **learn efficient codings of unlabelled data (unsupervised learning)**. It does this by compressing input data into a **lower-dimensional representation**. It then decodes the original data from this compressed version. Through training, the goal is to find a lower-dimensional representation that retains the most important features of the input data. \n",
    "\n",
    "Auto-encoders are **not considered a generative model** because it only reconstructs the given input. Also, auto-encoders are not used in real data compression tasks, due to their uncertainty and data dependency nature.  \n",
    "\n",
    "*Note: unsupervised learning is where the model learns patterns and structures in data, without outputs. In supervised learning, you may be learning the mapping between an input and output, or the outputs may give yu knowledge about the data's meaning. Whereas in supervised learning, a model learns the underlying patters of the dataset 'on its own'.*\n",
    "\n",
    "**Applications**:\n",
    "- Dimensionality reduction\n",
    "- Image denoising\n",
    "- Feature extraction\n",
    "- Data compression\n",
    "- Anomoly detection\n",
    "\n",
    "**Architecture:**\n",
    "- Encoder - e.g, CNN, FCNN\n",
    "- Bottleneck (latent space)\n",
    "- Decoder - e.g, CNN, FCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../images/autoencoder_illustration.png\" alt=\"Description\" width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many variants of auto-encoders, whcih are applied to many different problems. Common loss functions are **MSE** and **BCE**. Optimisation is achieved through **back-propogation and gradient descent**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  How to make a good auto-encoder?\n",
    "- **Keep the number of hidden layers low:** by doing this, the network is forced to learn the most important features of the data. \n",
    "- **Regularisation:** L1 or L2 regularisation can add a penalty term to the loss function, preventing the network from overfitting. This discourages large weights. \n",
    "- **Denoising:** we can add random noise to the input data during training. This forces the network to remove this noise during reconstruction and makes the model more robust.\n",
    "- **Tuning activation functions:** we can modify activation functions to encourage sparsity in the hidden layres, where only a few neurons are active at a time."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
